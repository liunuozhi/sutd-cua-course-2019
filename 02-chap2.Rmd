# EDA for text contents

Initially, the project will scoping into wikipedia articles at first, by only looking at the texts for now. 

```{r message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
library(tidytext)
library(tmap)
library(sf)
library(textstem)

wiki_hex <- read_rds(here::here("project-data/wiki_hex.rds"))
hex <- read_rds(here::here("project-data/hex.rds"))

theme_light() %>% theme_set()
```

## Tidy text

### A look at titles
```{r}
titles <- wiki_hex %>% st_set_geometry(NULL) %>% select(-text)
```

```{r message=FALSE, warning=FALSE}
my_stop_words <- tibble(word = c("station", "school", "mrt", "singapore"))

titles %>% 
  unnest_tokens(word, title) %>% 
  anti_join(stop_words) %>%
  # anti_join(my_stop_words) %>%
  count(word, sort = T) %>% 
  top_n(n=30) %>% 
  mutate(word = fct_reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  coord_flip()
```

From the titles, it can tell most of the locations are about stations and schools. There are 645 titles, while arount 100 titles are about stations and mrt, and another about 80 titles are about schools.

### A look at an articles

Here we can take a look a slot of articles. Here just randomly take the passage about Tuas Biomedical Park.

```{r}
wiki_text <- wiki_hex %>% st_set_geometry(NULL) # only text
print(wiki_text[2,]$text)
```

```{r}
wiki_text %>% mutate(word_number = lengths(str_split(text, pattern = " "))) %>% arrange(word_number) %>% skimr::skim()
```

```{r}
wiki_text %>% mutate(word_number = lengths(str_split(text, pattern = " "))) %>% 
  ggplot() + geom_histogram(aes(x = word_number, y = ..density..))
```

With skimming at the dataset, each articles contains 604 words in averrage. 50% of articles are over 360 words.

### Tokenizations & Lemmatization & POS tagging

A token is a meaningful unit of text, such as a word, that we are interested in using for analysis, and tokenization is the process of splitting text into tokens. On a simply way, tokenization means break a sentence into words and filter out the unimportant information which usually refers to words such as `the`, `a`, `that`, `so` as well as punctuation. 

However, tokenization may not promise a clean dataset, since some word in different format and tense will be counted respectively. For example, `located` and `locate`, `schools` and `school` and so on. Hence, it will require to apply lemmatization to extract the stem of the words in order to reduce the interference.

Also, it will be good if it can tell us whether is a noun or verb, here comes the pos tagging. 

```{r message=FALSE, warning=FALSE}
tokens <- wiki_text %>% group_by(hex_id) %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words) %>% ungroup() %>%
  mutate(word = lemmatize_words(word)) %>%  # lemmatize 
  inner_join(parts_of_speech) # pos tag
glimpse(tokens)
```

```{r}
skimr::skim(tokens)
```

```{r message=FALSE, warning=FALSE}
### Token
# token (freq > 1) count in total
tokens %>%
  count(word, sort = TRUE) %>% filter(n > 1)%>% nrow() %>% cat("There are ", ., "tokens (freq > 1).")

tokens %>%
  count(word, sort = TRUE) %>% filter(n > 1) %>% 
  ggplot(aes(n, ..density..)) + geom_histogram() + 
  labs(title = "tokens frequency (> 1)")

### Pos tag
# How the pos tag distributed?
tokens %>% 
  ggplot(aes(fct_infreq(pos))) + 
  geom_bar() + 
  coord_flip() +
  labs(x = "pos tag", title = "POS tag frequency")

# What is the most occurrin words?
tokens %>%
  count(word, sort = TRUE) %>% 
  top_n(n=30) %>% 
  mutate(word = fct_reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  coord_flip() + 
  labs(x = "word", 
       title = "Top 30 frequent words")

# What is the most occurrin adhectuve or adverb?
tokens %>% 
  filter(pos == "Adjective" |
         pos == "Adverb") %>% 
  count(word, sort = TRUE) %>% 
  top_n(n=30) %>% 
  mutate(word = fct_reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  coord_flip() + 
  labs(x = "Adj. / Adv.",
       title = "Most frequent adjective/ adverb")

# token %>% count(word, sort = T) %>% 
#   filter(n > 1) %>% 
#   wordcloud2::wordcloud2(color = "random-light")
```

With tokenizing and lemmatizing all the articles, there are 641707 words with excluding stopwords, and 14060 unique words remaining. There are 3584 words only occur once in the tokens (remaining 10476). 

The articles consist of a large portion of Noun. The interjection just takes a very small part. From the top 30 words, some key words shows significant ranks such as `school`, `station`, `road`, `park`, `centre`. Also, the most frequent adjective/adverb for descripiton would be directions (`east`, `north`, `south`), nationality and ethnicity (`national`, `japanese`, `chinese`, `british`) and so on. 

```{r message=FALSE, warning=FALSE}
key_word_filter = function(kw) {
  d <- hex %>% left_join(tokens %>% filter(word == kw) %>% group_by(hex_id) %>% count(word, sort = TRUE))
  
  tm_shape(d) + tm_polygons(col = "n") + tm_layout(title = kw)
}

key_word_filter("singapore")
key_word_filter("school")
key_word_filter("station")
key_word_filter("road")
key_word_filter("park")
key_word_filter("centre")
key_word_filter("design")
key_word_filter("national")
key_word_filter("chinese")
key_word_filter("public")
```

From the plots above, the spatial distribution is more dispersed, while other tokens are more condensed in central areas. 

## N-gram

Instead of just looking at one word, there are more common that some phrases occur in pairs. Splitting into one word may break the context of the semantics. In order to catch that, N-gram will help to certain sequence of words arrangement. 

### Bigram

```{r}
bigrams <- wiki_text %>% group_by(hex_id) %>% 
  unnest_tokens(bigram, text, token="ngrams", n = 2) %>% ungroup()

bigrams %>% separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word) %>% 
  unite(bigram, word1, word2, sep=" ") %>% 
  count(bigram, sort = T) %>% 
  top_n(30) %>% 
  ggplot(aes(x = fct_reorder(bigram, n), y = n)) + geom_col() +
  coord_flip() +
  labs(title = "Bigram Top 30", x = "Bigram")
```


We can see the relation in pair via `bigram`. 

It captures the phrase intead just breaking into two different tokens. Here gives the phrase like "pink dot", "junior college", "speakers corner", "mrt station" and so on. While some terms like "Ang Mo Kio", "Lee Kuan Yew" consisting of three words has been mistaken, since the bigram just capture two words and giving "chu kang", "kuan yew" and "lee kuan".

```{r}
trigrams <- wiki_text %>% group_by(hex_id) %>% 
  unnest_tokens(trigram, text, token="ngrams", n = 3, , n_min=1) %>% ungroup()
  
trigrams_filter <- trigrams %>% 
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %>% 
  mutate(word1 = lemmatize_words(word1)) %>%  # lemmatize 
  mutate(word2 = lemmatize_words(word2)) %>%  # lemmatize 
  mutate(word3 = lemmatize_words(word3)) %>%  # lemmatize 
  mutate(word2 = replace_na(word2, "")) %>% 
  mutate(word3 = replace_na(word3, ""))

trigrams_filter %>% filter(word2 != "", word3 != "") %>% count(word1, word2, word3, sort=T)

trigrams_united <- trigrams_filter %>% 
  unite(gram, word1, word2, word3, sep=" ") %>% 
  mutate(
    gram = str_trim(gram)
  )
trigrams_united %>% count(gram, sort=T) %>% 
  mutate(len = lengths(str_split(gram, " "))) %>% 
  filter(len == 3) %>% 
    top_n(30, n) %>% 
    ggplot(aes(x = fct_reorder(gram, n), y = n)) + geom_col() +
    coord_flip() +
    labs(title = "Trigram Top 30", x = "Trigram")
```

### Trigram

It makes more sense to have trigram which gives us some term "ang mo kio", "central business district", "choa chu kang" and so on. 

```{r}
# trigrams_united %>% saveRDS(here::here("project-data/trigram.rds"))
# tokens %>% saveRDS(here::here("project-data/token.rds"))
```

