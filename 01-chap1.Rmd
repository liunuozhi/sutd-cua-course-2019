# Prepare data

```{r message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
library(tmap)
library(sf)
```

## Wikipedia data

### change singapore to hexagons

The hexagon divides singapre as a unit of 1 km. Each hexagon grid will be attributed unique id as `hex_id`. 

```{r}
# singapore 
singapore <- st_read(here::here("project-data/singapore_outline/singapore_outline.shp")) %>% st_transform(crs=4326)

# hex
hex <- singapore %>% st_transform(crs = 3414) %>% st_make_grid(cellsize = 1000, square = F) %>% st_transform(crs=4326) %>% st_sf()

hex$hex_id <- hex %>% rownames() 

qtm(hex)
```

```{r}
cat("The total number of hexagons: ", hex %>% nrow())
glimpse(hex)
```

### Remove locations outside Singapore

```{r message=FALSE, warning=FALSE}
wiki <- read_csv(here::here("project-data/wiki.csv")) %>% select(-X1)
glimpse(wiki)

# wiki -> sf
wiki_sf <- wiki %>% st_as_sf(coords=c("lon", "lat"), crs=4326)

# How many points out of boundary?
wiki_sf_sg <- wiki_sf %>% st_filter(hex) # filter wiki sf

wiki_not_in_boundary <- wiki$title %>% setdiff(wiki_sf_sg$title)
wiki_not_in_boundary %>% length() %>% cat("There are ", ., "points out of the boundary.")

map_sg <- tm_shape(hex) + tm_polygons()
map_sg + tm_shape(wiki_sf) + tm_dots() + tm_layout(title = "Before filter")
map_sg + tm_shape(wiki_sf_sg) + tm_dots() + tm_layout(title = "After filter")
```

The dataset consists of 1233 wikipedia pages of locations. Each locations has its geocoordinate, page id, title of the page and the content of the article. From the glimpse of the data, we can see the dataset contains 5 columns: `lat`, `lon`, `pageid`, `title`, `text`. There are 1233 articles in the total datasets. 

At very primary look at the dataset, most of locations are close to central area. There are 108 points may locate in Malaysia which drop outside of the Singapore boundary. Here filters the dataset which only includes locations within Singapore. As as result, there are 1140 locations with Wikipedia pages. 

### binding points with hex_id

In this step, each locations will be assign a hex_id by checking the point is within the hexagons. 

```{r message=FALSE, warning=FALSE}
wiki_sf_sg <- st_join(wiki_sf_sg, hex)
```

```{r}
qtm(hex) + 
wiki_sf_sg %>% 
  count(hex_id, sort = T) %>% 
  top_n(20, n) %>% 
  qtm() + tm_layout(title = "Top 20 hex")
```

There are 381 hexagons which have been assigned to each wikipedia locations, while there are 1140 wikipedia locations in total.The plot above shows the top 20 hexagons which contain the most number of points. From the map, we can see central area has high density of listed location on Wikipeda. Besides some few hexagons show other parts Singapore also have relative dense locations on Wikipedia such as Tampinese, Sengkang and Punngol, Ang Mo Kio, Queestown and Jurong East. 

-----

## Housing 

### import housing location

```{r}
house <- read_rds(here::here("data/resale_with_geom.rds"))
```

### binding housing to hexagon

```{r message=FALSE, warning=FALSE}
house <- st_join(house %>% st_transform(crs = 4326), hex)
```

```{r}
qtm(hex) + house %>% sample_n(1000) %>% qtm()
```


```{r}
house_hex <- house %>% st_set_geometry(NULL) %>% group_by(hex_id) %>% 
  summarise(mean_price = mean(resale_price),
            mean_area = mean(floor_area_sqm),
            mean_sqr_price = mean_price / mean_area,
            mean_lease = mean(remaining_lease))

hex %>% left_join(house_hex) %>% tm_shape() + tm_polygons(col = "mean_sqr_price")
```

The same processing as the wikipedia page, the houses locations are aggregated by the same hexagons. Each hexagons will contain an average housing price for per meter squre. It is not surprsing to see the central area will be the most expensive. 

## Wiki in conjunction with House

```{r message=FALSE, warning=FALSE}
house_hex <- hex %>% left_join(house_hex) %>% filter(!is.na(mean_sqr_price))
```

```{r}
# wiki_hex <- st_filter(wiki_sf_sg, house_hex)
wiki_hex <- wiki_sf_sg
qtm(house_hex) + qtm(wiki_hex)
```

Since we need to examine the relation between wiki description and housing prices. It will require to filter out some locations outsdie the hexagons of housing. 

## Export data

```{r}
# hex %>% saveRDS(here::here("project-data/hex.rds"))
# wiki_sf_sg %>% saveRDS(here::here("project-data/wiki_sf_sg.rds")) # wiki page with hex id
# 
# house_hex %>% saveRDS(here::here("project-data/house_hex.rds"))
# wiki_hex %>% saveRDS(here::here("project-data/wiki_hex.rds"))
```
