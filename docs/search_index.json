[
["index.html", "Explore Wikipedia locations Chapter 1 Exploring Topics of Place and Housing Price 1.1 Introduction 1.2 Research question and hypothesis 1.3 Data sources 1.4 Methodology 1.5 Extensions/Limitations", " Explore Wikipedia locations Ronnie Liu Nuozhi Chapter 1 Exploring Topics of Place and Housing Price 1.1 Introduction Wikipedia is an online encyclopedia created and maintained as an open collaboration project by volunteer editors. It features exclusively free content and no commercial ads, and is owned and supported by the Wikimedia Foundation, a non-profit organization funded primarily through donations. (Wikipedia Contributors, 2018) Moreover, Wikipedia is one of the most visited sites on the web, offering a unique, entirely open, collaborative editing process based on contribution from experts and reviewers. The articles and cross-references provide general knowledge of topics. (Medelyan, 2009) Wikipedia contains description of places, becoming neutral reference to understanding the context, instead of involving subjective attachment to specific locations. This research is inspired by de Palma (de Palma, 2015), whose team addresses the endogenous choice process of employment location, which has correlation to real estate development. In this sense, the proposal targets endogeneity on spatially attributed topics in urban context, considered as part of choosing process. Then, it will explore in-depth relation between topics and housing price by computational methods. As part of the quantitative analysis, the methods will scope into Wikipedia content of describing Singapore locations. The objective of research is to differentiate the key information from the impartial description of urban places via quantitative approach, in order to indicate the relation between housing price and different topics of places. 1.2 Research question and hypothesis 1.2.1 Hypothesis The hypothesis of research is that the different Wikipedia description of urban places have correlation to housing price in Singapore. 1.2.2 Research question Do different topics in urban area affect housing price in Singapore? Sub-questions How different between the description of location in Singapore? Will the difference of description be related to housing price? 1.3 Data sources 1.3.1 Source 1: HDB housing Singapore HDB housing resale price from course materials. 1.3.2 Source 2: Wikipedia Data Source: The data were retrieved from Media Wiki API (MediaWiki, 2019) which is widely used by varied websites, companies and organizations, including Wikipedia. It is free and open to make knowledge available to people by providing searchable content, so that it is easy to retrieve archived information. Data retrieval: The first step is to use Geo-search to collect page title which refer to Singapore location. The radius of searching point will be 500 meters and divide Singapore into 4000 points. Once finishing searching and reduce the duplicated titles, there remains 1233 places, with having Wikipedia page, in Singapore. It contains title name, latitude and longitude, as showing in figure 1. figure 1 The location list can be used for search Wikipedia page content and extract the description of each location. The data sample shows as figure 2. figure 2 1.4 Methodology Initial natural language process will be applied to Wikipedia datasets, including tokenizing and building corpus. To understanding more about the difference on topic of the articles, a topic model will be applied to dataset. The expected outcome will be location tagged by different topic labels which will be extracted from trained classifiers. Later, the data will be transforming into spatial cells. Principle Component Analysis will be used to scale down the data, so that the dimensions of word vectors can be reduced. Then, as further examine the dataset by regression, it will be investigated whether the location tagged by topic with itself is positive or negative autocorrelation. Finally, the spatial regression will be used to understand how much the housing price related to topic-attributed places. 1.5 Extensions/Limitations The natural language process will be extended into the research project. The limitation will be the topic tags generated by pre-trained classifiers from other research works. Bibliography de Palma, A., Motamedi, K., Picard, N., &amp; Waddell, P. (2005). A model of residential location choice with endogenous housing prices and traffic for the Paris region. MediaWiki. (2019). Mediawiki.Org. https://www.mediawiki.org/wiki/MediaWiki Medelyan, O., Milne, D., Legg, C., &amp; Witten, I. H. (2009). Mining meaning from Wikipedia. International Journal of Human-Computer Studies, 67(9), 716-754. Wikipedia Contributors. (2018). Wikipedia. Retrieved from Wikipedia website: https://en.wikipedia.org/wiki/Wikipedia "],
["prepare-data.html", "Chapter 2 Prepare data 2.1 Wikipedia data 2.2 Housing 2.3 Wiki in conjunction with House 2.4 Export data", " Chapter 2 Prepare data 2.1 Wikipedia data 2.1.1 change singapore to hexagons The hexagon divides singapre as a unit of 1 km. Each hexagon grid will be attributed unique id as hex_id. # singapore singapore &lt;- st_read(here::here(&quot;project-data/singapore_outline/singapore_outline.shp&quot;)) %&gt;% st_transform(crs=4326) ## Reading layer `singapore_outline&#39; from data source `/Users/yaya/Desktop/Liu/liu-nuozhi/project-data/singapore_outline/singapore_outline.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 1 feature and 1 field ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33 ## proj4string: +proj=tmerc +lat_0=1.366666666666667 +lon_0=103.8333333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +datum=WGS84 +units=m +no_defs # hex hex &lt;- singapore %&gt;% st_transform(crs = 3414) %&gt;% st_make_grid(cellsize = 1000, square = F) %&gt;% st_transform(crs=4326) %&gt;% st_sf() hex$hex_id &lt;- hex %&gt;% rownames() qtm(hex) cat(&quot;The total number of hexagons: &quot;, hex %&gt;% nrow()) ## The total number of hexagons: 1141 glimpse(hex) ## Rows: 1,141 ## Columns: 2 ## $ geometry &lt;POLYGON [°]&gt; POLYGON ((103.6057 1.216125..., POLYGON ((103.6057 1… ## $ hex_id &lt;chr&gt; &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;7&quot;, &quot;8&quot;, &quot;9&quot;, &quot;10&quot;, &quot;11&quot;, &quot;12… 2.1.2 Remove locations outside Singapore wiki &lt;- read_csv(here::here(&quot;project-data/wiki.csv&quot;)) %&gt;% select(-X1) glimpse(wiki) ## Rows: 1,233 ## Columns: 5 ## $ lat &lt;dbl&gt; 1.232600, 1.366347, 1.334167, 1.451389, 1.376631, 1.211700, 1.… ## $ lon &lt;dbl&gt; 103.5327, 103.5484, 103.5494, 103.5889, 103.5918, 103.6125, 10… ## $ pageid &lt;dbl&gt; 13108718, 1838457, 6170311, 5073671, 5974898, 3956978, 6110331… ## $ title &lt;chr&gt; &quot;Masjid Al-Mawaddah&quot;, &quot;Port of Tanjung Pelepas&quot;, &quot;Pulai River&quot;… ## $ text &lt;chr&gt; &quot;Masjid Al-Mawaddah is a mosque located in Sengkang, at the j… # wiki -&gt; sf wiki_sf &lt;- wiki %&gt;% st_as_sf(coords=c(&quot;lon&quot;, &quot;lat&quot;), crs=4326) # How many points out of boundary? wiki_sf_sg &lt;- wiki_sf %&gt;% st_filter(hex) # filter wiki sf wiki_not_in_boundary &lt;- wiki$title %&gt;% setdiff(wiki_sf_sg$title) wiki_not_in_boundary %&gt;% length() %&gt;% cat(&quot;There are &quot;, ., &quot;points out of the boundary.&quot;) ## There are 93 points out of the boundary. map_sg &lt;- tm_shape(hex) + tm_polygons() map_sg + tm_shape(wiki_sf) + tm_dots() + tm_layout(title = &quot;Before filter&quot;) map_sg + tm_shape(wiki_sf_sg) + tm_dots() + tm_layout(title = &quot;After filter&quot;) The dataset consists of 1233 wikipedia pages of locations. Each locations has its geocoordinate, page id, title of the page and the content of the article. From the glimpse of the data, we can see the dataset contains 5 columns: lat, lon, pageid, title, text. There are 1233 articles in the total datasets. At very primary look at the dataset, most of locations are close to central area. There are 108 points may locate in Malaysia which drop outside of the Singapore boundary. Here filters the dataset which only includes locations within Singapore. As as result, there are 1140 locations with Wikipedia pages. 2.1.3 binding points with hex_id In this step, each locations will be assign a hex_id by checking the point is within the hexagons. wiki_sf_sg &lt;- st_join(wiki_sf_sg, hex) qtm(hex) + wiki_sf_sg %&gt;% count(hex_id, sort = T) %&gt;% top_n(20, n) %&gt;% qtm() + tm_layout(title = &quot;Top 20 hex&quot;) There are 381 hexagons which have been assigned to each wikipedia locations, while there are 1140 wikipedia locations in total.The plot above shows the top 20 hexagons which contain the most number of points. From the map, we can see central area has high density of listed location on Wikipeda. Besides some few hexagons show other parts Singapore also have relative dense locations on Wikipedia such as Tampinese, Sengkang and Punngol, Ang Mo Kio, Queestown and Jurong East. 2.2 Housing 2.2.1 import housing location house &lt;- read_rds(here::here(&quot;data/resale_with_geom.rds&quot;)) 2.2.2 binding housing to hexagon house &lt;- st_join(house %&gt;% st_transform(crs = 4326), hex) qtm(hex) + house %&gt;% sample_n(1000) %&gt;% qtm() house_hex &lt;- house %&gt;% st_set_geometry(NULL) %&gt;% group_by(hex_id) %&gt;% summarise(mean_price = mean(resale_price), mean_area = mean(floor_area_sqm), mean_sqr_price = mean_price / mean_area, mean_lease = mean(remaining_lease)) hex %&gt;% left_join(house_hex) %&gt;% tm_shape() + tm_polygons(col = &quot;mean_sqr_price&quot;) ## Joining, by = &quot;hex_id&quot; The same processing as the wikipedia page, the houses locations are aggregated by the same hexagons. Each hexagons will contain an average housing price for per meter squre. It is not surprsing to see the central area will be the most expensive. 2.3 Wiki in conjunction with House house_hex &lt;- hex %&gt;% left_join(house_hex) %&gt;% filter(!is.na(mean_sqr_price)) # wiki_hex &lt;- st_filter(wiki_sf_sg, house_hex) wiki_hex &lt;- wiki_sf_sg qtm(house_hex) + qtm(wiki_hex) Since we need to examine the relation between wiki description and housing prices. It will require to filter out some locations outsdie the hexagons of housing. 2.4 Export data # hex %&gt;% saveRDS(here::here(&quot;project-data/hex.rds&quot;)) # wiki_sf_sg %&gt;% saveRDS(here::here(&quot;project-data/wiki_sf_sg.rds&quot;)) # wiki page with hex id # # house_hex %&gt;% saveRDS(here::here(&quot;project-data/house_hex.rds&quot;)) # wiki_hex %&gt;% saveRDS(here::here(&quot;project-data/wiki_hex.rds&quot;)) "],
["eda-for-text-contents.html", "Chapter 3 EDA for text contents 3.1 Tidy text 3.2 N-gram", " Chapter 3 EDA for text contents Initially, the project will scoping into wikipedia articles at first, by only looking at the texts for now. 3.1 Tidy text 3.1.1 A look at titles titles &lt;- wiki_hex %&gt;% st_set_geometry(NULL) %&gt;% select(-text) my_stop_words &lt;- tibble(word = c(&quot;station&quot;, &quot;school&quot;, &quot;mrt&quot;, &quot;singapore&quot;)) titles %&gt;% unnest_tokens(word, title) %&gt;% anti_join(stop_words) %&gt;% # anti_join(my_stop_words) %&gt;% count(word, sort = T) %&gt;% top_n(n=30) %&gt;% mutate(word = fct_reorder(word, n)) %&gt;% ggplot(aes(word, n)) + geom_col() + coord_flip() From the titles, it can tell most of the locations are about stations and schools. There are 645 titles, while arount 100 titles are about stations and mrt, and another about 80 titles are about schools. 3.1.2 A look at an articles Here we can take a look a slot of articles. Here just randomly take the passage about Tuas Biomedical Park. wiki_text &lt;- wiki_hex %&gt;% st_set_geometry(NULL) # only text print(wiki_text[2,]$text) ## [1] &quot;Tuas Biomedical Park (abbreviation: TBP) is a biomedical manufacturing cluster developed by JTC Corporation at the western end of Singapore. The 183-hectare Tuas Biomedical Park I and 188-hectare Tuas Biomedical Park II are located at Tuas View – 20 minutes away from Jurong Port and five minutes from the Tuas Checkpoint to Malaysia. TBP comes with all essential infrastructure, such as roads, power lines, telecommunication lines, sewer pipes and water and gas supplies. Third parties are providing utilities such as steam, natural gas, chilled water and waste treatment services. To improve the appearance of the industrial estate, JTC has spent $6 million on lush landscaping. With the estate&#39;s \\&quot;plug-and-play\\&quot; design, pharmaceutical, biologics, medical device and other biomedical companies can set up manufacturing operations with minimal lead time. They can either move into fully serviced facilities or custom-build their own manufacturing plants. Currently, there are many well-known companies operating in TBP. They include global biomedical players like Merck, Novartis, Pfizer, Wyeth, Genentech and GlaxoSmithKline.&quot; wiki_text %&gt;% mutate(word_number = lengths(str_split(text, pattern = &quot; &quot;))) %&gt;% arrange(word_number) %&gt;% skimr::skim() Table 3.1: Data summary Name Piped data Number of rows 1140 Number of columns 5 _______________________ Column type frequency: character 3 numeric 2 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace title 0 1 5 90 0 1140 0 text 0 1 79 80106 0 1140 0 hex_id 0 1 1 4 0 381 0 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist pageid 0 1 16851182.49 17973887.17 27318 2644529 8263434 28053295.00 63144025 ▇▂▂▁▁ word_number 0 1 659.65 927.28 13 204 383 753.75 12469 ▇▁▁▁▁ wiki_text %&gt;% mutate(word_number = lengths(str_split(text, pattern = &quot; &quot;))) %&gt;% ggplot() + geom_histogram(aes(x = word_number, y = ..density..)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. With skimming at the dataset, each articles contains 604 words in averrage. 50% of articles are over 360 words. 3.1.3 Tokenizations &amp; Lemmatization &amp; POS tagging A token is a meaningful unit of text, such as a word, that we are interested in using for analysis, and tokenization is the process of splitting text into tokens. On a simply way, tokenization means break a sentence into words and filter out the unimportant information which usually refers to words such as the, a, that, so as well as punctuation. However, tokenization may not promise a clean dataset, since some word in different format and tense will be counted respectively. For example, located and locate, schools and school and so on. Hence, it will require to apply lemmatization to extract the stem of the words in order to reduce the interference. Also, it will be good if it can tell us whether is a noun or verb, here comes the pos tagging. tokens &lt;- wiki_text %&gt;% group_by(hex_id) %&gt;% unnest_tokens(word, text) %&gt;% anti_join(stop_words) %&gt;% ungroup() %&gt;% mutate(word = lemmatize_words(word)) %&gt;% # lemmatize inner_join(parts_of_speech) # pos tag glimpse(tokens) ## Rows: 641,707 ## Columns: 5 ## $ pageid &lt;dbl&gt; 3956978, 3956978, 3956978, 3956978, 3956978, 3956978, 3956978,… ## $ title &lt;chr&gt; &quot;Malaysia–Singapore border&quot;, &quot;Malaysia–Singapore border&quot;, &quot;Mal… ## $ hex_id &lt;chr&gt; &quot;5&quot;, &quot;5&quot;, &quot;5&quot;, &quot;5&quot;, &quot;5&quot;, &quot;5&quot;, &quot;5&quot;, &quot;5&quot;, &quot;5&quot;, &quot;5&quot;, &quot;5&quot;, &quot;5&quot;, &quot;5… ## $ word &lt;chr&gt; &quot;malaysia&quot;, &quot;singapore&quot;, &quot;border&quot;, &quot;border&quot;, &quot;international&quot;, … ## $ pos &lt;chr&gt; &quot;Noun&quot;, &quot;Noun&quot;, &quot;Noun&quot;, &quot;Verb (transitive)&quot;, &quot;Adjective&quot;, &quot;Nou… skimr::skim(tokens) Table 3.2: Data summary Name tokens Number of rows 641707 Number of columns 5 _______________________ Column type frequency: character 4 numeric 1 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace title 0 1.00 5 90 0 1140 0 hex_id 0 1.00 1 4 0 381 0 word 0 1.00 1 20 0 14060 0 pos 9977 0.98 4 21 0 13 0 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist pageid 0 1 13125193 16462102 27318 1920900 4398470 17072954 63144025 ▇▂▁▁▁ ### Token # token (freq &gt; 1) count in total tokens %&gt;% count(word, sort = TRUE) %&gt;% filter(n &gt; 1)%&gt;% nrow() %&gt;% cat(&quot;There are &quot;, ., &quot;tokens (freq &gt; 1).&quot;) ## There are 10476 tokens (freq &gt; 1). tokens %&gt;% count(word, sort = TRUE) %&gt;% filter(n &gt; 1) %&gt;% ggplot(aes(n, ..density..)) + geom_histogram() + labs(title = &quot;tokens frequency (&gt; 1)&quot;) ### Pos tag # How the pos tag distributed? tokens %&gt;% ggplot(aes(fct_infreq(pos))) + geom_bar() + coord_flip() + labs(x = &quot;pos tag&quot;, title = &quot;POS tag frequency&quot;) # What is the most occurrin words? tokens %&gt;% count(word, sort = TRUE) %&gt;% top_n(n=30) %&gt;% mutate(word = fct_reorder(word, n)) %&gt;% ggplot(aes(word, n)) + geom_col() + coord_flip() + labs(x = &quot;word&quot;, title = &quot;Top 30 frequent words&quot;) # What is the most occurrin adhectuve or adverb? tokens %&gt;% filter(pos == &quot;Adjective&quot; | pos == &quot;Adverb&quot;) %&gt;% count(word, sort = TRUE) %&gt;% top_n(n=30) %&gt;% mutate(word = fct_reorder(word, n)) %&gt;% ggplot(aes(word, n)) + geom_col() + coord_flip() + labs(x = &quot;Adj. / Adv.&quot;, title = &quot;Most frequent adjective/ adverb&quot;) # token %&gt;% count(word, sort = T) %&gt;% # filter(n &gt; 1) %&gt;% # wordcloud2::wordcloud2(color = &quot;random-light&quot;) With tokenizing and lemmatizing all the articles, there are 641707 words with excluding stopwords, and 14060 unique words remaining. There are 3584 words only occur once in the tokens (remaining 10476). The articles consist of a large portion of Noun. The interjection just takes a very small part. From the top 30 words, some key words shows significant ranks such as school, station, road, park, centre. Also, the most frequent adjective/adverb for descripiton would be directions (east, north, south), nationality and ethnicity (national, japanese, chinese, british) and so on. key_word_filter = function(kw) { d &lt;- hex %&gt;% left_join(tokens %&gt;% filter(word == kw) %&gt;% group_by(hex_id) %&gt;% count(word, sort = TRUE)) tm_shape(d) + tm_polygons(col = &quot;n&quot;) + tm_layout(title = kw) } key_word_filter(&quot;singapore&quot;) key_word_filter(&quot;school&quot;) key_word_filter(&quot;station&quot;) key_word_filter(&quot;road&quot;) key_word_filter(&quot;park&quot;) key_word_filter(&quot;centre&quot;) key_word_filter(&quot;design&quot;) key_word_filter(&quot;national&quot;) key_word_filter(&quot;chinese&quot;) key_word_filter(&quot;public&quot;) From the plots above, the spatial distribution is more dispersed, while other tokens are more condensed in central areas. 3.2 N-gram Instead of just looking at one word, there are more common that some phrases occur in pairs. Splitting into one word may break the context of the semantics. In order to catch that, N-gram will help to certain sequence of words arrangement. 3.2.1 Bigram bigrams &lt;- wiki_text %&gt;% group_by(hex_id) %&gt;% unnest_tokens(bigram, text, token=&quot;ngrams&quot;, n = 2) %&gt;% ungroup() bigrams %&gt;% separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;% filter(!word1 %in% stop_words$word) %&gt;% filter(!word2 %in% stop_words$word) %&gt;% unite(bigram, word1, word2, sep=&quot; &quot;) %&gt;% count(bigram, sort = T) %&gt;% top_n(30) %&gt;% ggplot(aes(x = fct_reorder(bigram, n), y = n)) + geom_col() + coord_flip() + labs(title = &quot;Bigram Top 30&quot;, x = &quot;Bigram&quot;) ## Selecting by n We can see the relation in pair via bigram. It captures the phrase intead just breaking into two different tokens. Here gives the phrase like “pink dot”, “junior college”, “speakers corner”, “mrt station” and so on. While some terms like “Ang Mo Kio”, “Lee Kuan Yew” consisting of three words has been mistaken, since the bigram just capture two words and giving “chu kang”, “kuan yew” and “lee kuan”. trigrams &lt;- wiki_text %&gt;% group_by(hex_id) %&gt;% unnest_tokens(trigram, text, token=&quot;ngrams&quot;, n = 3, , n_min=1) %&gt;% ungroup() trigrams_filter &lt;- trigrams %&gt;% separate(trigram, c(&quot;word1&quot;, &quot;word2&quot;, &quot;word3&quot;), sep = &quot; &quot;) %&gt;% filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word, !word3 %in% stop_words$word) %&gt;% mutate(word1 = lemmatize_words(word1)) %&gt;% # lemmatize mutate(word2 = lemmatize_words(word2)) %&gt;% # lemmatize mutate(word3 = lemmatize_words(word3)) %&gt;% # lemmatize mutate(word2 = replace_na(word2, &quot;&quot;)) %&gt;% mutate(word3 = replace_na(word3, &quot;&quot;)) ## Warning: Expected 3 pieces. Missing pieces filled with `NA` in 1520344 rows [1, ## 2, 4, 5, 7, 8, 10, 11, 13, 14, 16, 17, 19, 20, 22, 23, 25, 26, 28, 29, ...]. trigrams_filter %&gt;% filter(word2 != &quot;&quot;, word3 != &quot;&quot;) %&gt;% count(word1, word2, word3, sort=T) ## # A tibble: 73,058 x 4 ## word1 word2 word3 n ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 singapore cambridge gce 239 ## 2 mass rapid transit 154 ## 3 ang mo kio 129 ## 4 rapid transit mrt 126 ## 5 world war ii 117 ## 6 east west line 114 ## 7 gce ordinary level 110 ## 8 choa chu kang 108 ## 9 lee kuan yew 107 ## 10 cambridge gce normal 106 ## # … with 73,048 more rows trigrams_united &lt;- trigrams_filter %&gt;% unite(gram, word1, word2, word3, sep=&quot; &quot;) %&gt;% mutate( gram = str_trim(gram) ) trigrams_united %&gt;% count(gram, sort=T) %&gt;% mutate(len = lengths(str_split(gram, &quot; &quot;))) %&gt;% filter(len == 3) %&gt;% top_n(30, n) %&gt;% ggplot(aes(x = fct_reorder(gram, n), y = n)) + geom_col() + coord_flip() + labs(title = &quot;Trigram Top 30&quot;, x = &quot;Trigram&quot;) 3.2.2 Trigram It makes more sense to have trigram which gives us some term “ang mo kio”, “central business district”, “choa chu kang” and so on. # trigrams_united %&gt;% saveRDS(here::here(&quot;project-data/trigram.rds&quot;)) # tokens %&gt;% saveRDS(here::here(&quot;project-data/token.rds&quot;)) "],
["topic-modeling.html", "Chapter 4 Topic modeling 4.1 Corpus 4.2 TF-IDF 4.3 LDA 4.4 Binding topics with spatial information", " Chapter 4 Topic modeling In this stage, the trigram will be the input dataset (Corpus), it will be converted into a document-term matrix (DTM) while in this case, the document will refer to the hexagon which collect the Wikipedia articles. The question is how can to categorazie the documents (hexgons) into different topics. For answering this questino , the project turns to topic model for a potential solution. 4.1 Corpus word_count &lt;- trigram %&gt;% count(hex_id, gram, sort = T) dtm &lt;- cast_dtm(data = word_count, term = gram, document = hex_id, value = n) dtm_tidy &lt;- tidy(dtm) my_stop_words &lt;- tibble(term = c(&quot;building&quot;, &quot;build&quot;, &quot;street&quot;, &quot;singapore&quot;, &quot;mrt&quot;, &quot;station&quot;, &quot;mass rapid&quot;, &quot;rapid transit&quot;, &quot;singapore&#39;s&quot;, &quot;transit mrt&quot;, &quot;underground mass&quot;, &quot;school&#39;s&quot;, &quot;bus&quot;, &quot;interchange&quot;, &quot;marina&quot;, &quot;bay&quot;)) term_more_then_two &lt;- dtm_tidy %&gt;% count(term) %&gt;% filter(n &gt; 1) %&gt;% pluck(&quot;term&quot;) dtm_tidy %&gt;% filter(term %in% term_more_then_two) %&gt;% # remove term only appear once filter(!str_detect(term, &quot;\\\\d&quot;)) %&gt;% # filter number anti_join(my_stop_words) %&gt;% count(term, sort = T) %&gt;% ggplot(aes(n)) + geom_histogram(bins = 100) dtm_clean &lt;- dtm_tidy %&gt;% filter(term %in% term_more_then_two) %&gt;% # remove term only appear once filter(!str_detect(term, &quot;\\\\d&quot;)) %&gt;% # filter number anti_join(my_stop_words) dtm_clean %&gt;% count(term, sort = T) ## # A tibble: 28,654 x 2 ## term n ## &lt;chr&gt; &lt;int&gt; ## 1 locate 325 ## 2 include 248 ## 3 road 247 ## 4 house 216 ## 5 serve 214 ## 6 start 206 ## 7 land 196 ## 8 service 192 ## 9 centre 190 ## 10 park 187 ## # … with 28,644 more rows dtm &lt;- dtm_clean %&gt;% cast_dtm(document, term, count) 4.2 TF-IDF In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It will calculate the weight of importance in the corpus via term frequency and inverse document frequency. The higher tf-idf, the more signicant the word which contribute to the document classifications. ### TF-IDF tfidf &lt;- bind_tf_idf(tbl = dtm_tidy, term = term, document = document, n = count) %&gt;% arrange(desc(tf_idf)) tfidf %&gt;% mutate(rank = row_number()) %&gt;% ggplot(aes(rank, tf_idf)) + geom_line() However, the plot above show a highly distorted curve which may indicate that the terms may split into two extreme. It either contribute to nothing since most documents contain the word, or the word only appear in a extremely low frequency. In advance, the research will seek for other models. 4.3 LDA Topic model is one of popular model applied for natural language processing. It is also called Latent Dirichlet allocation. It assumed that every document is a mixture of topics, and every topic is a mixture of words. The model will estimate both at the same time in order to predict the possible topic. For now, the topic number is set to 8 after few rouds of manual selections. k &lt;- 8 wiki_lda &lt;- LDA(dtm, k = k, method = &quot;Gibbs&quot;, control = list(seed = 1234)) wiki_lda ## A LDA_Gibbs topic model with 8 topics. top_terms &lt;- tidy(wiki_lda, matrix=&quot;beta&quot;) %&gt;% group_by(topic) %&gt;% top_n(10, beta) %&gt;% ungroup() %&gt;% arrange(topic, -beta) top_terms %&gt;% mutate(term = reorder_within(term, beta, topic)) %&gt;% ggplot(aes(term, beta, fill = factor(topic))) + geom_col(show.legend = FALSE) + facet_wrap(~ topic, scales = &quot;free&quot;) + coord_flip() + scale_x_reordered() top_terms &lt;- tidy(wiki_lda, matrix=&quot;beta&quot;) %&gt;% group_by(topic) %&gt;% top_n(100, beta) %&gt;% ungroup() %&gt;% arrange(topic, -beta) top_terms %&gt;% pivot_wider( names_from = topic, values_from = beta ) %&gt;% replace(is.na(.), 0) %&gt;% ggparcoord(., columns = 1:k, alpha = 0.2) The model returns relatively better result. It can be inferred that there are some distinctions between each topics. The labels are my interpretation according to the words consisting the topics. Topic 1 2 3 4 5 6 7 8 Label Landmark Public service Recreation University School Important buildings Transport Culture Topic 1: it relates to airport, stadium, marina bay, sports and so on. This topic can be treated as certain landmarks in the city. Topic 2: it associates with police, hospital, service and so on. Regard to that, the label may be the public service. Topic 3: it mentions parks, reservoir, gardens frequently. Based on those keyword, it will be treated as recreational places. Topic 4 &amp; Topic 5: both topics indicate the educational area, while there some nuances which illustrate that topic 4 is more inclined to tertiary education, with topic 5 tending to be the primary education. Topic 6: this category consists of court, parliament, art, museum. They are some important building in the city. Topic 7: this topic is associated with transportation, with suggesting mrt station, line, service, center. Topic 8: the words are about cultural element such as chinese, temple, mosque and so on. assignments &lt;- augment(wiki_lda, data = dtm) assignments &lt;- assignments %&gt;% select(document, .topic) %&gt;% distinct(document, .keep_all = T) colnames(assignments)[2] &lt;- &quot;topic&quot; assignments %&gt;% ggplot() + geom_histogram(aes(x=topic, y=..density..), binwidth = 1) 4.3.1 Composition of topics (theta) Theta is one of the posterior items generated by the model is the per document probabilities of the topics, which represent a distribution of topics over documents. The result displayed is the theta for all topics from all spatial hex units. theta = posterior(wiki_lda)$topics %&gt;% as.data.frame() %&gt;% mutate(document = rownames(.)) %&gt;% as_tibble() theta %&gt;% head() ## # A tibble: 6 x 9 ## `1` `2` `3` `4` `5` `6` `7` `8` document ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.0292 0.00394 0.0179 0.0448 0.00392 0.773 0.00204 0.125 723 ## 2 0.0343 0.0205 0.0170 0.0125 0.00202 0.157 0.00545 0.751 706 ## 3 0.116 0.0461 0.319 0.178 0.0125 0.212 0.00515 0.111 640 ## 4 0.00935 0.0129 0.0549 0.207 0.0108 0.456 0.0219 0.227 707 ## 5 0.00825 0.0120 0.0116 0.198 0.681 0.0668 0.00215 0.0204 553 ## 6 0.0128 0.0252 0.0717 0.0473 0.00743 0.125 0.00349 0.707 690 4.4 Binding topics with spatial information wiki_hex &lt;- read_rds(here::here(&quot;project/data/wiki_hex.rds&quot;)) hex &lt;- read_rds(here::here(&quot;project/data/rds/hex.rds&quot;)) tmap_mode(&quot;view&quot;) ## tmap mode set to interactive viewing # tm_shape(hex) + tm_polygons(col = &quot;#ffffff&quot;, aalpha = 0.5) + hex %&gt;% left_join(assignments, by = c(&quot;hex_id&quot; = &quot;document&quot;)) %&gt;% filter(!is.na(topic)) %&gt;% mutate(topic = paste0(&quot;topic&quot;, topic)) %&gt;% tm_shape() + tm_polygons(col = &quot;topic&quot;, alpha = 0.5) # + tm_shape(wiki_hex %&gt;% select(-pageid)) + tm_dots(alpha=0.5) tmap_mode(&quot;plot&quot;) ## tmap mode set to plotting From the map, it can tell the central area is dominant by topic 6 (important buildings) with some are topic 8 (culture). Topic 5 (primary education) are more relatively equally distributed in to parts of city, since primary enducation is important to be accessible. While topic 7 (transportation) are more related to few locations like Tampinese and Woodlands. Topic 4 (university) is obviously attached with NTU, NUS, SMU, SUTD and other colleges. assignments %&gt;% saveRDS(here::here(&quot;project-data/topic.rds&quot;)) dtm_clean %&gt;% saveRDS(here::here(&quot;project-data/dtm.rds&quot;)) theta %&gt;% saveRDS(here::here(&quot;project-data/theta.rds&quot;)) "],
["pca.html", "Chapter 5 PCA 5.1 Binding topic with housing 5.2 Sample statistic 5.3 PCA 5.4 Regression", " Chapter 5 PCA theme_light() %&gt;% theme_set() house_hex &lt;- read_rds(here::here(&quot;project-data/house_hex.rds&quot;)) theta &lt;- read_rds(here::here(&quot;project-data/theta.rds&quot;)) topic &lt;- read_rds(here::here(&quot;project-data/topic.rds&quot;)) %&gt;% mutate(topic = paste(&quot;topic&quot;,topic, sep = &quot;_&quot;)) 5.1 Binding topic with housing This setup is to bind topics with housing, which will filter out some locations with topics but without housing informaiton. It opted out the topic 2 (puublic service) which does not appear in the housing datasets. topic &lt;- topic %&gt;% left_join(theta) ## Joining, by = &quot;document&quot; colnames(topic) &lt;- paste( &quot;topic&quot;, colnames(topic), sep = &quot;_&quot;) house_hex &lt;- house_hex %&gt;% left_join(topic, by = c(&quot;hex_id&quot; = &quot;topic_document&quot;)) %&gt;% filter(!is.na(topic_topic)) %&gt;% rename(!!&quot;topic&quot;:=topic_topic) house_hex %&gt;% tm_shape() + tm_polygons(col = &quot;topic&quot;) bind_data &lt;- house_hex %&gt;% st_set_geometry(NULL) bind_data %&gt;% head() ## hex_id mean_price mean_area mean_sqr_price mean_lease topic topic_1 ## 1 167 346765.6 105.20312 3296.153 70.42188 topic_4 0.04075092 ## 2 193 365440.9 101.13976 3613.227 75.32289 topic_6 0.24845406 ## 3 205 395574.2 88.94545 4447.379 90.05455 topic_7 0.07038835 ## 4 206 390642.3 117.43452 3326.469 74.70387 topic_5 0.06833333 ## 5 218 455775.8 106.09975 4295.730 82.93682 topic_7 0.03409091 ## 6 219 378439.8 107.05645 3534.956 82.97984 topic_5 0.03346995 ## topic_2 topic_3 topic_4 topic_5 topic_6 topic_7 topic_8 ## 1 0.07554945 0.02609890 0.57188645 0.07371795 0.10851648 0.0279304 0.07554945 ## 2 0.03202297 0.01435512 0.18750000 0.13361307 0.21400177 0.1274293 0.04262367 ## 3 0.08980583 0.08980583 0.07038835 0.08009709 0.07038835 0.4684466 0.06067961 ## 4 0.06166667 0.06166667 0.05500000 0.42833333 0.06833333 0.1550000 0.10166667 ## 5 0.03409091 0.03106061 0.04924242 0.03106061 0.04015152 0.7522727 0.02803030 ## 6 0.27937158 0.04166667 0.03346995 0.21926230 0.02800546 0.3203552 0.04439891 5.2 Sample statistic skim(bind_data) Table 5.1: Data summary Name bind_data Number of rows 193 Number of columns 14 _______________________ Column type frequency: character 2 numeric 12 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace hex_id 0 1 3 3 0 193 0 topic 0 1 7 7 0 7 0 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist mean_price 0 1 461830.42 105766.21 260333.33 386485.98 438532.77 523440.25 787926.03 ▂▇▃▂▁ mean_area 0 1 95.04 14.79 57.00 84.14 95.36 106.00 127.36 ▂▅▇▇▃ mean_sqr_price 0 1 4916.61 1114.77 3296.15 4043.05 4589.79 5482.73 8770.76 ▇▇▃▂▁ mean_lease 0 1 70.80 9.05 50.22 64.38 69.22 77.27 95.36 ▂▇▆▅▁ topic_1 0 1 0.07 0.07 0.01 0.03 0.05 0.08 0.51 ▇▁▁▁▁ topic_2 0 1 0.08 0.13 0.01 0.03 0.04 0.08 0.81 ▇▁▁▁▁ topic_3 0 1 0.07 0.11 0.00 0.02 0.04 0.07 0.63 ▇▁▁▁▁ topic_4 0 1 0.08 0.11 0.01 0.03 0.05 0.09 0.75 ▇▁▁▁▁ topic_5 0 1 0.24 0.28 0.00 0.03 0.08 0.48 0.89 ▇▁▁▂▁ topic_6 0 1 0.08 0.09 0.01 0.03 0.05 0.09 0.73 ▇▁▁▁▁ topic_7 0 1 0.24 0.24 0.00 0.05 0.14 0.39 0.88 ▇▂▂▁▁ topic_8 0 1 0.13 0.16 0.01 0.04 0.07 0.15 0.75 ▇▁▁▁▁ bind_data %&gt;% ggplot(aes(fct_reorder(topic, mean_sqr_price), mean_sqr_price)) + geom_boxplot() The chart and statistics summary above show that there some price-topic relation. Topic 4 (university) shows higher average housing prices. Topic 6 (important buildings) has second highest average housing price. While the housing near topic 8 (culture) has the least housing price. 5.2.1 Compare mean price between topics The hypothesis testing takes topic 5 (School) and topic 6 (Important buildings) as comparison. The null hypothesis assumes that the topics have a same housing price, which is mean(topic5) - mean(topic6) = 0. topic_mean &lt;- bind_data %&gt;% filter(topic %in% c(&quot;topic_5&quot;, &quot;topic_6&quot;)) %&gt;% group_by(topic) %&gt;% summarise(mean = mean(mean_sqr_price)) topic_mean ## # A tibble: 2 x 2 ## topic mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 topic_5 4791. ## 2 topic_6 5378. ggplot() + geom_histogram( data = bind_data %&gt;% filter(topic %in% c(&quot;topic_5&quot;, &quot;topic_6&quot;)), mapping = aes(x = mean_sqr_price, group = topic, fill = topic), position = position_dodge() ) + geom_vline( data = topic_mean, mapping = aes(xintercept = mean, group = topic, color = topic) ) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. bind_data %&gt;% filter(topic == &quot;topic_5&quot;) %&gt;% specify(response = mean_sqr_price) %&gt;% generate(reps = 100) %&gt;% calculate(stat = &quot;mean&quot;) %&gt;% get_ci() ## Setting `type = &quot;bootstrap&quot;` in `generate()`. ## # A tibble: 1 x 2 ## `2.5%` `97.5%` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 4598. 5070. mean_diff &lt;- bind_data %&gt;% filter(topic %in% c(&quot;topic_5&quot;, &quot;topic_6&quot;)) %&gt;% specify(formula = mean_sqr_price ~ topic) %&gt;% calculate(stat = &quot;diff in means&quot;, order = c(&quot;topic_5&quot;, &quot;topic_6&quot;)) mean_diff ## # A tibble: 1 x 1 ## stat ## &lt;dbl&gt; ## 1 -587. null_distribution &lt;- bind_data %&gt;% filter(topic %in% c(&quot;topic_5&quot;, &quot;topic_6&quot;)) %&gt;% specify(formula = mean_sqr_price ~ topic) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 500, type = &quot;permute&quot;) %&gt;% calculate(stat = &quot;diff in means&quot;, order = c(&quot;topic_5&quot;, &quot;topic_6&quot;)) null_ci &lt;- null_distribution %&gt;% get_ci() null_distribution %&gt;% visualise(bins = 100) + shade_p_value(obs_stat = mean_diff, direction = &quot;both&quot;) + shade_ci(null_ci) null_distribution %&gt;% get_pvalue(obs_stat = mean_diff, direction = &quot;both&quot;) ## # A tibble: 1 x 1 ## p_value ## &lt;dbl&gt; ## 1 0 The significance is set to 0.05. In this case, the p-value is smaller than 0.05. So, it rejects H0, and indicates that the average price of topic 5 is significantly different from that of topic 6. 5.3 PCA topic_house &lt;- bind_data topic_house %&gt;% select(starts_with(&quot;topic_&quot;)) %&gt;% head() ## topic_1 topic_2 topic_3 topic_4 topic_5 topic_6 topic_7 ## 1 0.04075092 0.07554945 0.02609890 0.57188645 0.07371795 0.10851648 0.0279304 ## 2 0.24845406 0.03202297 0.01435512 0.18750000 0.13361307 0.21400177 0.1274293 ## 3 0.07038835 0.08980583 0.08980583 0.07038835 0.08009709 0.07038835 0.4684466 ## 4 0.06833333 0.06166667 0.06166667 0.05500000 0.42833333 0.06833333 0.1550000 ## 5 0.03409091 0.03409091 0.03106061 0.04924242 0.03106061 0.04015152 0.7522727 ## 6 0.03346995 0.27937158 0.04166667 0.03346995 0.21926230 0.02800546 0.3203552 ## topic_8 ## 1 0.07554945 ## 2 0.04262367 ## 3 0.06067961 ## 4 0.10166667 ## 5 0.02803030 ## 6 0.04439891 To understand more about the dataset, it will use PCA to see how the topics incorporate with each other. The input data with be the theta value of each hexagons (shows above output). The theta value indicates how the data fit with each topics and shows the probabilities. pc &lt;- topic_house %&gt;% # select(-mean_price, -mean_area, -topic) %&gt;% select(starts_with(&quot;topic_&quot;)) %&gt;% prcomp(., scale. = T, center = T) tidy(pc, &quot;pcs&quot;) ## # A tibble: 8 x 4 ## PC std.dev percent cumulative ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1.32e+ 0 0.216 0.216 ## 2 2 1.18e+ 0 0.175 0.391 ## 3 3 1.05e+ 0 0.138 0.529 ## 4 4 1.01e+ 0 0.128 0.657 ## 5 5 9.91e- 1 0.123 0.780 ## 6 6 9.64e- 1 0.116 0.896 ## 7 7 9.12e- 1 0.104 1 ## 8 8 5.25e-16 0 1 tidy(pc, &quot;pcs&quot;) %&gt;% ggplot(aes(x = PC, y = percent)) + geom_line() + geom_text(aes(x = PC, y = percent, label = PC), nudge_y = 0.03) tidy(pc, &quot;pcs&quot;) %&gt;% ggplot(aes(x = PC, y = cumulative)) + geom_hline(yintercept = 0.8, color = &quot;orange&quot;) + geom_line() + geom_text(aes(x = PC, y = cumulative, label = PC), nudge_y = 0.03) tidy(pc, &quot;variables&quot;) %&gt;% filter(PC &lt; 6) %&gt;% # only show first 4 components ggplot(aes(x = column, y = value)) + geom_hline(yintercept = 0) + geom_col(aes(fill=(value &gt;= 0)), show.legend = FALSE) + coord_flip() + facet_grid(~PC) In order to catch 80% of variance, it will set to see the first 5 princle components. Even thought the dimension do not reduce a lot, it provides a scope to see how the topics related. From the first component, the topic 4 (university) and 5 (schools) have negative relations with other topics. With some nuance, topic 7 will be less likely to align with others, which shows in PC2. autoplot(pc, label = TRUE, x = 1, y = 2, loadings = T, loadings.label = T, label.size = 3) autoplot(pc, label = TRUE, x = 3, y = 4, loadings = T, loadings.label = T, label.size = 3) From PC1 and PC2 dimension, most topics are close to each other except topic 4, 5, 7. It shows that topic 4 (university) and topic 7 (transport) have negative relations. Topic 5 (schools) also shows relatively negative relation with topic 7. From PC3 and PC4, it shows that topic 4 (university) shows either negative or no relation with other topics. A possible reason may be a university can be a isolated area with self functioning place like a university town. The same feature also appear to topic 7 (transport), because the transportation interchange may take a large area land where other functional places may not be able to be included. Topic 1 (landmark), 3 (recreation), 5 (school) and 8 (culture) show a strong relation with each other, the reason may be that Singapore has well mixture of land use. pc_topic &lt;- topic_house %&gt;% augment(pc, .) %&gt;% select(hex_id, mean_sqr_price, .fittedPC1, .fittedPC2, .fittedPC3, .fittedPC4, .fittedPC5) pc_topic %&gt;% head() ## # A tibble: 6 x 7 ## hex_id mean_sqr_price .fittedPC1 .fittedPC2 .fittedPC3 .fittedPC4 .fittedPC5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 167 3296. -0.527 1.83 -2.46 2.37 -2.30 ## 2 193 3613. 0.944 0.775 -0.0504 1.76 -0.934 ## 3 205 4447. 0.448 -1.02 -0.298 0.0708 -0.170 ## 4 206 3326. -0.649 0.165 0.436 -0.200 -0.00827 ## 5 218 4296. 0.205 -2.35 -0.438 0.465 0.235 ## 6 219 3535. -0.288 -0.504 -1.21 -1.24 0.399 5.4 Regression ols &lt;- lm(mean_sqr_price ~ .fittedPC1+ .fittedPC2+ .fittedPC3+ .fittedPC4+ .fittedPC5, pc_topic) glance(ols) ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.148 0.126 1042. 6.52 1.30e-5 6 -1612. 3238. 3261. ## # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; The regression model take on the first 5 principle components which consisting of theta values. The R.squared shows a relative low relation between housing price and topics. tidy(ols) ## # A tibble: 6 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 4917. 75.0 65.5 6.19e-131 ## 2 .fittedPC1 161. 57.2 2.81 5.42e- 3 ## 3 .fittedPC2 278. 63.7 4.36 2.11e- 5 ## 4 .fittedPC3 1.57 71.6 0.0220 9.82e- 1 ## 5 .fittedPC4 126. 74.4 1.69 9.27e- 2 ## 6 .fittedPC5 126. 75.9 1.66 9.77e- 2 A potential interpretation of the regression model may be that the PC1 and PC2 will have higher estimated housing prices, which the divergence of schols/university/transport and other locations being cateched. This divergence intensify the housing price fluctuation. In other words, if the locations tend to be either educational area or others, the housing prices may show a relative increase or decrease. topic_house_ols &lt;- augment(ols, data = topic_house) topic_house_ols ## # A tibble: 193 x 21 ## hex_id mean_price mean_area mean_sqr_price mean_lease topic topic_1 topic_2 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 167 346766. 105. 3296. 70.4 topi… 0.0408 0.0755 ## 2 193 365441. 101. 3613. 75.3 topi… 0.248 0.0320 ## 3 205 395574. 88.9 4447. 90.1 topi… 0.0704 0.0898 ## 4 206 390642. 117. 3326. 74.7 topi… 0.0683 0.0617 ## 5 218 455776. 106. 4296. 82.9 topi… 0.0341 0.0341 ## 6 219 378440. 107. 3535. 83.0 topi… 0.0335 0.279 ## 7 246 495319. 109. 4529. 80.6 topi… 0.00719 0.0119 ## 8 261 295190. 73.5 4018. 61.9 topi… 0.045 0.025 ## 9 277 403684. 115. 3505. 66.8 topi… 0.108 0.125 ## 10 290 374949. 101. 3729. 70.8 topi… 0.336 0.152 ## # … with 183 more rows, and 13 more variables: topic_3 &lt;dbl&gt;, topic_4 &lt;dbl&gt;, ## # topic_5 &lt;dbl&gt;, topic_6 &lt;dbl&gt;, topic_7 &lt;dbl&gt;, topic_8 &lt;dbl&gt;, .fitted &lt;dbl&gt;, ## # .se.fit &lt;dbl&gt;, .resid &lt;dbl&gt;, .hat &lt;dbl&gt;, .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;, ## # .std.resid &lt;dbl&gt; "],
["conclusion-and-limitations.html", "Chapter 6 Conclusion and limitations 6.1 Conclusion 6.2 Limitations", " Chapter 6 Conclusion and limitations 6.1 Conclusion The project starts with asking two questions: How different between the description of location in Singapore? Will the difference of description be related to housing price? In order to answer these two, the project takes Wikipedia and housing price datasets as objectives. Initially, the text mining on Wikipedia provides a big picture of the dataset where schools take a signicant amount of the portions. Then, the document-term matrix generated from n-gram is inputed into topic models. Since TF-IDF returns a relative bad result, the project took LDA method to compute 8 topics which can be interpretated as landmark, public service, recreation, university, school, important buildings, transport and culture. With these topic in conjunction with housing price, the PCA and regression model provied a scope of how different topics incorporate with each other. A potental explaination will be that educational and transport have certain divergence with other types of locations. This divergence may cause the housing price fluctuation. 6.2 Limitations Although the result provides a certain conclusion and inspiration, since the educational places take a signicant portion of corpus. The topic generated from the Wikipedia dataset may contain some bias on certain types of location, in this case which is school. Besides, the principle components on theta value of topic model have been taken into regression model, which may not include a very clear predictable varibles. "]
]
