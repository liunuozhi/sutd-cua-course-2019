---
title: "1-EDA"
output: 
  github_document:
    toc: true
    toc_depth: 3
---

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(tidytext)
library(tmap)
library(sf)
library(textstem)

wiki_hex <- read_rds(here::here("project/data/wiki_hex.rds"))
hex <- read_rds(here::here("project/data/rds/hex.rds"))
```

```{r}
theme_light() %>% theme_set()
```

## Tidy text

### A look at titles
```{r}
titles <- wiki_hex %>% st_set_geometry(NULL) %>% select(-text)
```

```{r message=FALSE, warning=FALSE}
my_stop_words <- tibble(word = c("station", "school", "mrt", "singapore"))

titles %>% 
  unnest_tokens(word, title) %>% 
  anti_join(stop_words) %>%
  # anti_join(my_stop_words) %>%
  count(word, sort = T) %>% 
  top_n(n=30) %>% 
  mutate(word = fct_reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  coord_flip()
```

From the titles, it can tell most of the locations are about stations and schools. There are 645 titles, while arount 100 titles are about stations and mrt, and another about 80 titles are about schools.

### A look at an articles

Here we can take a look a slot of articles. Just take the first passage.

```{r}
wiki_text <- wiki_hex %>% st_set_geometry(NULL) # only text
print(wiki_text[1,]$text)
```

```{r}
wiki_text %>% mutate(word_number = lengths(str_split(text, pattern = " "))) %>% arrange(word_number) %>% skimr::skim()
```

```{r}
wiki_text %>% mutate(word_number = lengths(str_split(text, pattern = " "))) %>% 
  ggplot() + geom_histogram(aes(x = word_number, y = ..density..))
```

With skimming at the dataset, each articles contains 604 words in averrage. 50% of articles are over 360 words.

### Tokenizations & Lemmatization & POS tagging

```{r message=FALSE, warning=FALSE}
tokens <- wiki_text %>% group_by(hex_id) %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words) %>% ungroup() %>%
  mutate(word = lemmatize_words(word)) %>%  # lemmatize 
  inner_join(parts_of_speech) # pos tag
glimpse(tokens)
```

```{r}
skimr::skim(tokens)
```

```{r message=FALSE, warning=FALSE}
### Token
# token (freq > 1) count in total
tokens %>%
  count(word, sort = TRUE) %>% filter(n > 1)%>% nrow() %>% cat("There are ", ., "tokens (freq > 1).")

tokens %>%
  count(word, sort = TRUE) %>% filter(n > 1) %>% 
  ggplot(aes(n, ..density..)) + geom_histogram() + 
  labs(title = "tokens frequency (> 1)")

### Pos tag
# How the pos tag distributed?
tokens %>% 
  ggplot(aes(fct_infreq(pos))) + 
  geom_bar() + 
  coord_flip() +
  labs(x = "pos tag", title = "POS tag frequency")

# What is the most occurrin words?
tokens %>%
  count(word, sort = TRUE) %>% 
  top_n(n=30) %>% 
  mutate(word = fct_reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  coord_flip() + 
  labs(x = "word", 
       title = "Top 30 frequent words")

# What is the most occurrin adhectuve or adverb?
tokens %>% 
  filter(pos == "Adjective" |
         pos == "Adverb") %>% 
  count(word, sort = TRUE) %>% 
  top_n(n=30) %>% 
  mutate(word = fct_reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  coord_flip() + 
  labs(x = "Adj. / Adv.",
       title = "Most frequent adjective/ adverb")
```

With tokenizing and lemmatizing all the articles, there are 212257 words without excluding stopwords. After remove stop words, there are 10485 unique words remaining. There are 7711 words only occur once in the tokens (remaining 9695). 

From the top 30 words, we can see the word `singapore` and `singapore's` take a large proportion of the data. Some key words shows significant ranks such as `school`, `station`, `road`, `park`, `centre`.

```{r message=FALSE, warning=FALSE}
key_word_filter = function(kw) {
  d <- hex %>% left_join(tokens %>% filter(word == kw) %>% group_by(hex_id) %>% count(word, sort = TRUE))
  
  tm_shape(d) + tm_polygons(col = "n") + tm_layout(title = kw)
}

key_word_filter("singapore")
key_word_filter("school")
key_word_filter("station")
key_word_filter("road")
key_word_filter("park")
key_word_filter("centre")
key_word_filter("design")
key_word_filter("national")
key_word_filter("chinese")
key_word_filter("public")
```

### N-gram

**Bigram**

```{r}
bigrams <- wiki_text %>% group_by(hex_id) %>% 
  unnest_tokens(bigram, text, token="ngrams", n = 2) %>% ungroup()

bigrams %>% separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word) %>% 
  unite(bigram, word1, word2, sep=" ") %>% 
  count(bigram, sort = T) %>% 
  top_n(30) %>% 
  ggplot(aes(x = fct_reorder(bigram, n), y = n)) + geom_col() +
  coord_flip() +
  labs(title = "Bigram Top 30", x = "Bigram")
```


We can see the relation between two words via `n-gram`. 

It captures the phrase intead just breaking into two different tokens. Here gives the phrase like "pink dot", "junior college", "speakers corner", "mrt station" and so on. While some terms like "Ang Mo Kio", "Lee Kuan Yew" consisting of three words has been mistaken, since the bigram just capture two words and giving "chu kang", "kuan yew" and "lee kuan".

```{r}
trigrams <- wiki_text %>% group_by(hex_id) %>% 
  unnest_tokens(trigram, text, token="ngrams", n = 3, , n_min=1) %>% ungroup()
  
trigrams_filter <- trigrams %>% 
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %>% 
  mutate(word1 = lemmatize_words(word1)) %>%  # lemmatize 
  mutate(word2 = lemmatize_words(word2)) %>%  # lemmatize 
  mutate(word3 = lemmatize_words(word3)) %>%  # lemmatize 
  mutate(word2 = replace_na(word2, "")) %>% 
  mutate(word3 = replace_na(word3, ""))

trigrams_filter %>% filter(word2 != "", word3 != "") %>% count(word1, word2, word3, sort=T)

trigrams_united <- trigrams_filter %>% 
  unite(gram, word1, word2, word3, sep=" ") %>% 
  mutate(
    gram = str_trim(gram)
  )
trigrams_united %>% count(gram, sort=T) %>% 
  mutate(len = lengths(str_split(gram, " "))) %>% 
  filter(len == 3) %>% 
    top_n(30, n) %>% 
    ggplot(aes(x = fct_reorder(gram, n), y = n)) + geom_col() +
    coord_flip() +
    labs(title = "Trigram Top 30", x = "Trigram")
```

**Trigram**

It makes more sense to have trigram which gives us some term "ang mo kio", "central business district", "choa chu kang" and so on. 

```{r}
# trigrams_united %>% saveRDS("./data/trigram.rds")
# tokens %>% saveRDS("./data/token.rds")
```

